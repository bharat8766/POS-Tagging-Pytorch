{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\" Converts the sequence into list of indices mapped by to_ix.\n",
    "    \"\"\"\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "\n",
    "    def init_hidden(self):\n",
    "       \n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "\n",
    "    def report_accuracy(self, data, word_to_ix, tag_to_ix, ix_to_tag, print_data=False):\n",
    "        \"\"\" Reports accuracy with respect to exact match (all tags correct per sentence)\n",
    "            and total matches (all correctly classified tags).\n",
    "        \"\"\"\n",
    "        # Here we don't need to train, so the code is wrapped in torch.no_grad() \n",
    "        with torch.no_grad(): \n",
    "            total = 0 \n",
    "            total_correct = 0 \n",
    "            total_exact_correct = 0 \n",
    "            for sentence, tags in data: \n",
    "                scores = self(prepare_sequence(sentence, word_to_ix)) \n",
    "                out = torch.argmax(scores, dim=1) \n",
    "                out_tags = [ix_to_tag[ix] for ix in out] \n",
    "                targets = prepare_sequence(tags, tag_to_ix) \n",
    "     \n",
    "                correct = 0 \n",
    "                length = len(tags) \n",
    "                for i in range(length): \n",
    "                    if out[i] == targets[i]: \n",
    "                        correct += 1 \n",
    "\n",
    "                total += length\n",
    "                total_correct += correct\n",
    "\n",
    "\n",
    "            n = len(data)\n",
    "           \n",
    "            print('Accuracy: %d / %d, %0.4f' % (total_correct, total, total_correct / total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model and helper functions.\n",
    "from lstm import LSTMTagger, prepare_sequence\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# These will usually be 32 or 64 dimensional (little sense to go above 100).\n",
    "EMBEDDING_DIM = 64\n",
    "\n",
    "HIDDEN_DIM = 10\n",
    "\n",
    "# There will usually be more epochs; use 5 or lower to debug.\n",
    "EPOCHS = 5\n",
    "\n",
    "TAGS = [\"\", \"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\", \"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"MD\", \"NN\", \"NNP\", \"NNPS\", \"NNS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"SYM\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\", \"``\"]\n",
    "\n",
    "def main(train_file):\n",
    "    # Load the data.\n",
    "    training_data = read_data(train_file)\n",
    "    n = len(training_data)\n",
    "    ########\n",
    "    #print(n)\n",
    "\n",
    "    # Store word -> word_index mapping.\n",
    "    word_to_ix = {}\n",
    "    for sent, tags in training_data:\n",
    "        for word in sent:\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "    ##print(word_to_ix)\n",
    "\n",
    "    # Store tag -> tag_index mapping.\n",
    "    tag_to_ix = {tag: ix for ix, tag in enumerate(TAGS)}\n",
    "    ##################\n",
    "    #print(tag_to_ix)\n",
    "    ##################\n",
    "    \n",
    "    # Initialize the model.\n",
    "    model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i, (sentence, tags) in enumerate(training_data):\n",
    "            # Step 1. Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Also, we need to clear out the hidden state of the LSTM,\n",
    "            # detaching it from its history on the last instance.\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "            targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model(sentence_in)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            # calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "               \n",
    "                print('Epoch %d, sentence %d/%d, loss: %0.4f' % (epoch + 1, i + 1, n, loss))\n",
    "                \n",
    "    \n",
    "                \n",
    "\n",
    "    # Report training accuracy\n",
    "    model.report_accuracy(training_data, word_to_ix, tag_to_ix, TAGS)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, sentence 1/8936, loss: 3.7635\n",
      "Epoch 1, sentence 1001/8936, loss: 2.2525\n",
      "Epoch 1, sentence 2001/8936, loss: 2.0573\n",
      "Epoch 1, sentence 3001/8936, loss: 1.8925\n",
      "Epoch 1, sentence 4001/8936, loss: 1.1531\n",
      "Epoch 1, sentence 5001/8936, loss: 2.0201\n",
      "Epoch 1, sentence 6001/8936, loss: 1.8570\n",
      "Epoch 1, sentence 7001/8936, loss: 1.4255\n",
      "Epoch 1, sentence 8001/8936, loss: 1.1311\n",
      "Epoch 2, sentence 1/8936, loss: 1.1893\n",
      "Epoch 2, sentence 1001/8936, loss: 0.9903\n",
      "Epoch 2, sentence 2001/8936, loss: 1.3996\n",
      "Epoch 2, sentence 3001/8936, loss: 1.8867\n",
      "Epoch 2, sentence 4001/8936, loss: 0.8194\n",
      "Epoch 2, sentence 5001/8936, loss: 1.5147\n",
      "Epoch 2, sentence 6001/8936, loss: 1.3861\n",
      "Epoch 2, sentence 7001/8936, loss: 1.4181\n",
      "Epoch 2, sentence 8001/8936, loss: 0.8049\n",
      "Epoch 3, sentence 1/8936, loss: 1.0150\n",
      "Epoch 3, sentence 1001/8936, loss: 0.9189\n",
      "Epoch 3, sentence 2001/8936, loss: 1.3324\n",
      "Epoch 3, sentence 3001/8936, loss: 1.7707\n",
      "Epoch 3, sentence 4001/8936, loss: 0.7306\n",
      "Epoch 3, sentence 5001/8936, loss: 1.3531\n",
      "Epoch 3, sentence 6001/8936, loss: 1.1658\n",
      "Epoch 3, sentence 7001/8936, loss: 1.4730\n",
      "Epoch 3, sentence 8001/8936, loss: 0.6803\n",
      "Epoch 4, sentence 1/8936, loss: 0.8967\n",
      "Epoch 4, sentence 1001/8936, loss: 0.7665\n",
      "Epoch 4, sentence 2001/8936, loss: 1.3733\n",
      "Epoch 4, sentence 3001/8936, loss: 1.7657\n",
      "Epoch 4, sentence 4001/8936, loss: 0.6187\n",
      "Epoch 4, sentence 5001/8936, loss: 1.3109\n",
      "Epoch 4, sentence 6001/8936, loss: 1.0411\n",
      "Epoch 4, sentence 7001/8936, loss: 1.3218\n",
      "Epoch 4, sentence 8001/8936, loss: 0.5577\n",
      "Epoch 5, sentence 1/8936, loss: 0.8082\n",
      "Epoch 5, sentence 1001/8936, loss: 0.6731\n",
      "Epoch 5, sentence 2001/8936, loss: 1.3424\n",
      "Epoch 5, sentence 3001/8936, loss: 1.7102\n",
      "Epoch 5, sentence 4001/8936, loss: 0.5517\n",
      "Epoch 5, sentence 5001/8936, loss: 1.2437\n",
      "Epoch 5, sentence 6001/8936, loss: 1.0051\n",
      "Epoch 5, sentence 7001/8936, loss: 1.2620\n",
      "Epoch 5, sentence 8001/8936, loss: 0.4807\n",
      "Accuracy: 164334 / 211727, 0.7762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\" Reads a vertical corpus with two columns: word, pos-tag.\n",
    "        Returns: list of tuples: [(words, tags)], one record per sentence.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    with open(\"data/train.txt\") as datafile:\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in datafile:\n",
    "            line = line.rstrip()\n",
    "            if not line:\n",
    "                data.append((words, tags))\n",
    "                words = []\n",
    "                tags = []\n",
    "            else:\n",
    "                word, tag = line.split()\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "    \n",
    "    \n",
    "            \n",
    "    return data\n",
    "  \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) < 2:\n",
    "        print('Usage: tagger.py TRAINSET', file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    main(sys.argv[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
