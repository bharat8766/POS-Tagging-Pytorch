{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    \"\"\" Converts the sequence into list of indices mapped by to_ix.\n",
    "    \"\"\"\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "\n",
    "    def init_hidden(self):\n",
    "       \n",
    "        return (torch.zeros(1, 1, self.hidden_dim),\n",
    "                torch.zeros(1, 1, self.hidden_dim))\n",
    "\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(\n",
    "            embeds.view(len(sentence), 1, -1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "\n",
    "    def report_accuracy(self, data, word_to_ix, tag_to_ix, ix_to_tag, print_data=False):\n",
    "        \"\"\" Reports accuracy with respect to exact match (all tags correct per sentence)\n",
    "            and total matches (all correctly classified tags).\n",
    "        \"\"\"\n",
    "        # Here we don't need to train, so the code is wrapped in torch.no_grad() \n",
    "        with torch.no_grad(): \n",
    "            total = 0 \n",
    "            total_correct = 0 \n",
    "            total_exact_correct = 0 \n",
    "            for sentence, tags in data: \n",
    "                scores = self(prepare_sequence(sentence, word_to_ix)) \n",
    "                out = torch.argmax(scores, dim=1) \n",
    "                out_tags = [ix_to_tag[ix] for ix in out] \n",
    "                targets = prepare_sequence(tags, tag_to_ix) \n",
    "     \n",
    "                correct = 0 \n",
    "                length = len(tags) \n",
    "                for i in range(length): \n",
    "                    if out[i] == targets[i]: \n",
    "                        correct += 1 \n",
    "\n",
    "                total += length\n",
    "                total_correct += correct\n",
    "\n",
    "\n",
    "            n = len(data)\n",
    "           \n",
    "            print('Accuracy: %d / %d, %0.4f' % (total_correct, total, total_correct / total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our model and helper functions.\n",
    "from lstm import LSTMTagger, prepare_sequence\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# These will usually be 32 or 64 dimensional (little sense to go above 100).\n",
    "EMBEDDING_DIM = 64\n",
    "\n",
    "HIDDEN_DIM = 10\n",
    "\n",
    "# There will usually be more epochs; use 5 or lower to debug.\n",
    "EPOCHS = 60\n",
    "\n",
    "TAGS = [\"\", \"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\", \"CC\", \"CD\", \"DT\", \"EX\", \"FW\", \"IN\", \"JJ\", \"JJR\", \"JJS\", \"MD\", \"NN\", \"NNP\", \"NNPS\", \"NNS\", \"PDT\", \"POS\", \"PRP\", \"PRP$\", \"RB\", \"RBR\", \"RBS\", \"RP\", \"SYM\", \"TO\", \"UH\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\", \"WDT\", \"WP\", \"WP$\", \"WRB\", \"``\"]\n",
    "\n",
    "def main(train_file):\n",
    "    # Load the data.\n",
    "    training_data = read_data(train_file)\n",
    "    n = len(training_data)\n",
    "    ########\n",
    "    #print(n)\n",
    "\n",
    "    # Store word -> word_index mapping.\n",
    "    word_to_ix = {}\n",
    "    for sent, tags in training_data:\n",
    "        for word in sent:\n",
    "            if word not in word_to_ix:\n",
    "                word_to_ix[word] = len(word_to_ix)\n",
    "    ##print(word_to_ix)\n",
    "\n",
    "    # Store tag -> tag_index mapping.\n",
    "    tag_to_ix = {tag: ix for ix, tag in enumerate(TAGS)}\n",
    "    ##################\n",
    "    #print(tag_to_ix)\n",
    "    ##################\n",
    "    \n",
    "    # Initialize the model.\n",
    "    model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i, (sentence, tags) in enumerate(training_data):\n",
    "            # Step 1. Pytorch accumulates gradients.\n",
    "            # We need to clear them out before each instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Also, we need to clear out the hidden state of the LSTM,\n",
    "            # detaching it from its history on the last instance.\n",
    "            model.hidden = model.init_hidden()\n",
    "\n",
    "            # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "            # Tensors of word indices.\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "            targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "            # Step 3. Run our forward pass.\n",
    "            tag_scores = model(sentence_in)\n",
    "\n",
    "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "            # calling optimizer.step()\n",
    "            loss = loss_function(tag_scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 90 == 0:\n",
    "               \n",
    "                print('Epoch %d, sentence %d/%d, loss: %0.4f' % (epoch + 1, i + 1, n, loss))\n",
    "                \n",
    "    \n",
    "                \n",
    "\n",
    "    # Report training accuracy\n",
    "    model.report_accuracy(training_data, word_to_ix, tag_to_ix, TAGS)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, sentence 1/185, loss: 3.8362\n",
      "Epoch 1, sentence 91/185, loss: 3.4082\n",
      "Epoch 1, sentence 181/185, loss: 3.2384\n",
      "Epoch 2, sentence 1/185, loss: 2.9869\n",
      "Epoch 2, sentence 91/185, loss: 3.0002\n",
      "Epoch 2, sentence 181/185, loss: 2.9594\n",
      "Epoch 3, sentence 1/185, loss: 2.6437\n",
      "Epoch 3, sentence 91/185, loss: 2.7313\n",
      "Epoch 3, sentence 181/185, loss: 2.7602\n",
      "Epoch 4, sentence 1/185, loss: 2.4014\n",
      "Epoch 4, sentence 91/185, loss: 2.4764\n",
      "Epoch 4, sentence 181/185, loss: 2.6191\n",
      "Epoch 5, sentence 1/185, loss: 2.2118\n",
      "Epoch 5, sentence 91/185, loss: 2.2853\n",
      "Epoch 5, sentence 181/185, loss: 2.5221\n",
      "Epoch 6, sentence 1/185, loss: 2.0825\n",
      "Epoch 6, sentence 91/185, loss: 2.1688\n",
      "Epoch 6, sentence 181/185, loss: 2.4392\n",
      "Epoch 7, sentence 1/185, loss: 1.9961\n",
      "Epoch 7, sentence 91/185, loss: 2.0898\n",
      "Epoch 7, sentence 181/185, loss: 2.3596\n",
      "Epoch 8, sentence 1/185, loss: 1.9258\n",
      "Epoch 8, sentence 91/185, loss: 2.0292\n",
      "Epoch 8, sentence 181/185, loss: 2.2768\n",
      "Epoch 9, sentence 1/185, loss: 1.8569\n",
      "Epoch 9, sentence 91/185, loss: 1.9780\n",
      "Epoch 9, sentence 181/185, loss: 2.1898\n",
      "Epoch 10, sentence 1/185, loss: 1.7886\n",
      "Epoch 10, sentence 91/185, loss: 1.9328\n",
      "Epoch 10, sentence 181/185, loss: 2.1177\n",
      "Epoch 11, sentence 1/185, loss: 1.7342\n",
      "Epoch 11, sentence 91/185, loss: 1.8904\n",
      "Epoch 11, sentence 181/185, loss: 2.0488\n",
      "Epoch 12, sentence 1/185, loss: 1.6868\n",
      "Epoch 12, sentence 91/185, loss: 1.8473\n",
      "Epoch 12, sentence 181/185, loss: 1.9874\n",
      "Epoch 13, sentence 1/185, loss: 1.6464\n",
      "Epoch 13, sentence 91/185, loss: 1.8018\n",
      "Epoch 13, sentence 181/185, loss: 1.9311\n",
      "Epoch 14, sentence 1/185, loss: 1.6102\n",
      "Epoch 14, sentence 91/185, loss: 1.7557\n",
      "Epoch 14, sentence 181/185, loss: 1.8734\n",
      "Epoch 15, sentence 1/185, loss: 1.5723\n",
      "Epoch 15, sentence 91/185, loss: 1.7121\n",
      "Epoch 15, sentence 181/185, loss: 1.8136\n",
      "Epoch 16, sentence 1/185, loss: 1.5349\n",
      "Epoch 16, sentence 91/185, loss: 1.6715\n",
      "Epoch 16, sentence 181/185, loss: 1.7540\n",
      "Epoch 17, sentence 1/185, loss: 1.5014\n",
      "Epoch 17, sentence 91/185, loss: 1.6370\n",
      "Epoch 17, sentence 181/185, loss: 1.6967\n",
      "Epoch 18, sentence 1/185, loss: 1.4712\n",
      "Epoch 18, sentence 91/185, loss: 1.6066\n",
      "Epoch 18, sentence 181/185, loss: 1.6418\n",
      "Epoch 19, sentence 1/185, loss: 1.4435\n",
      "Epoch 19, sentence 91/185, loss: 1.5799\n",
      "Epoch 19, sentence 181/185, loss: 1.5895\n",
      "Epoch 20, sentence 1/185, loss: 1.4186\n",
      "Epoch 20, sentence 91/185, loss: 1.5555\n",
      "Epoch 20, sentence 181/185, loss: 1.5393\n",
      "Epoch 21, sentence 1/185, loss: 1.3965\n",
      "Epoch 21, sentence 91/185, loss: 1.5327\n",
      "Epoch 21, sentence 181/185, loss: 1.4912\n",
      "Epoch 22, sentence 1/185, loss: 1.3764\n",
      "Epoch 22, sentence 91/185, loss: 1.5114\n",
      "Epoch 22, sentence 181/185, loss: 1.4474\n",
      "Epoch 23, sentence 1/185, loss: 1.3564\n",
      "Epoch 23, sentence 91/185, loss: 1.4913\n",
      "Epoch 23, sentence 181/185, loss: 1.4081\n",
      "Epoch 24, sentence 1/185, loss: 1.3369\n",
      "Epoch 24, sentence 91/185, loss: 1.4725\n",
      "Epoch 24, sentence 181/185, loss: 1.3697\n",
      "Epoch 25, sentence 1/185, loss: 1.3176\n",
      "Epoch 25, sentence 91/185, loss: 1.4537\n",
      "Epoch 25, sentence 181/185, loss: 1.3315\n",
      "Epoch 26, sentence 1/185, loss: 1.2988\n",
      "Epoch 26, sentence 91/185, loss: 1.4342\n",
      "Epoch 26, sentence 181/185, loss: 1.2932\n",
      "Epoch 27, sentence 1/185, loss: 1.2816\n",
      "Epoch 27, sentence 91/185, loss: 1.4150\n",
      "Epoch 27, sentence 181/185, loss: 1.2540\n",
      "Epoch 28, sentence 1/185, loss: 1.2657\n",
      "Epoch 28, sentence 91/185, loss: 1.3963\n",
      "Epoch 28, sentence 181/185, loss: 1.2150\n",
      "Epoch 29, sentence 1/185, loss: 1.2500\n",
      "Epoch 29, sentence 91/185, loss: 1.3777\n",
      "Epoch 29, sentence 181/185, loss: 1.1770\n",
      "Epoch 30, sentence 1/185, loss: 1.2343\n",
      "Epoch 30, sentence 91/185, loss: 1.3583\n",
      "Epoch 30, sentence 181/185, loss: 1.1409\n",
      "Epoch 31, sentence 1/185, loss: 1.2191\n",
      "Epoch 31, sentence 91/185, loss: 1.3376\n",
      "Epoch 31, sentence 181/185, loss: 1.1062\n",
      "Epoch 32, sentence 1/185, loss: 1.2039\n",
      "Epoch 32, sentence 91/185, loss: 1.3162\n",
      "Epoch 32, sentence 181/185, loss: 1.0728\n",
      "Epoch 33, sentence 1/185, loss: 1.1881\n",
      "Epoch 33, sentence 91/185, loss: 1.2950\n",
      "Epoch 33, sentence 181/185, loss: 1.0411\n",
      "Epoch 34, sentence 1/185, loss: 1.1714\n",
      "Epoch 34, sentence 91/185, loss: 1.2746\n",
      "Epoch 34, sentence 181/185, loss: 1.0116\n",
      "Epoch 35, sentence 1/185, loss: 1.1534\n",
      "Epoch 35, sentence 91/185, loss: 1.2553\n",
      "Epoch 35, sentence 181/185, loss: 0.9840\n",
      "Epoch 36, sentence 1/185, loss: 1.1341\n",
      "Epoch 36, sentence 91/185, loss: 1.2359\n",
      "Epoch 36, sentence 181/185, loss: 0.9582\n",
      "Epoch 37, sentence 1/185, loss: 1.1140\n",
      "Epoch 37, sentence 91/185, loss: 1.2164\n",
      "Epoch 37, sentence 181/185, loss: 0.9340\n",
      "Epoch 38, sentence 1/185, loss: 1.0948\n",
      "Epoch 38, sentence 91/185, loss: 1.1963\n",
      "Epoch 38, sentence 181/185, loss: 0.9107\n",
      "Epoch 39, sentence 1/185, loss: 1.0784\n",
      "Epoch 39, sentence 91/185, loss: 1.1754\n",
      "Epoch 39, sentence 181/185, loss: 0.8880\n",
      "Epoch 40, sentence 1/185, loss: 1.0651\n",
      "Epoch 40, sentence 91/185, loss: 1.1537\n",
      "Epoch 40, sentence 181/185, loss: 0.8653\n",
      "Epoch 41, sentence 1/185, loss: 1.0543\n",
      "Epoch 41, sentence 91/185, loss: 1.1321\n",
      "Epoch 41, sentence 181/185, loss: 0.8434\n",
      "Epoch 42, sentence 1/185, loss: 1.0445\n",
      "Epoch 42, sentence 91/185, loss: 1.1110\n",
      "Epoch 42, sentence 181/185, loss: 0.8223\n",
      "Epoch 43, sentence 1/185, loss: 1.0348\n",
      "Epoch 43, sentence 91/185, loss: 1.0904\n",
      "Epoch 43, sentence 181/185, loss: 0.8015\n",
      "Epoch 44, sentence 1/185, loss: 1.0253\n",
      "Epoch 44, sentence 91/185, loss: 1.0702\n",
      "Epoch 44, sentence 181/185, loss: 0.7808\n",
      "Epoch 45, sentence 1/185, loss: 1.0162\n",
      "Epoch 45, sentence 91/185, loss: 1.0502\n",
      "Epoch 45, sentence 181/185, loss: 0.7602\n",
      "Epoch 46, sentence 1/185, loss: 1.0074\n",
      "Epoch 46, sentence 91/185, loss: 1.0305\n",
      "Epoch 46, sentence 181/185, loss: 0.7394\n",
      "Epoch 47, sentence 1/185, loss: 0.9993\n",
      "Epoch 47, sentence 91/185, loss: 1.0113\n",
      "Epoch 47, sentence 181/185, loss: 0.7187\n",
      "Epoch 48, sentence 1/185, loss: 0.9912\n",
      "Epoch 48, sentence 91/185, loss: 0.9925\n",
      "Epoch 48, sentence 181/185, loss: 0.6988\n",
      "Epoch 49, sentence 1/185, loss: 0.9831\n",
      "Epoch 49, sentence 91/185, loss: 0.9744\n",
      "Epoch 49, sentence 181/185, loss: 0.6804\n",
      "Epoch 50, sentence 1/185, loss: 0.9748\n",
      "Epoch 50, sentence 91/185, loss: 0.9568\n",
      "Epoch 50, sentence 181/185, loss: 0.6633\n",
      "Epoch 51, sentence 1/185, loss: 0.9661\n",
      "Epoch 51, sentence 91/185, loss: 0.9396\n",
      "Epoch 51, sentence 181/185, loss: 0.6481\n",
      "Epoch 52, sentence 1/185, loss: 0.9568\n",
      "Epoch 52, sentence 91/185, loss: 0.9229\n",
      "Epoch 52, sentence 181/185, loss: 0.6342\n",
      "Epoch 53, sentence 1/185, loss: 0.9466\n",
      "Epoch 53, sentence 91/185, loss: 0.9066\n",
      "Epoch 53, sentence 181/185, loss: 0.6217\n",
      "Epoch 54, sentence 1/185, loss: 0.9358\n",
      "Epoch 54, sentence 91/185, loss: 0.8906\n",
      "Epoch 54, sentence 181/185, loss: 0.6100\n",
      "Epoch 55, sentence 1/185, loss: 0.9244\n",
      "Epoch 55, sentence 91/185, loss: 0.8751\n",
      "Epoch 55, sentence 181/185, loss: 0.5991\n",
      "Epoch 56, sentence 1/185, loss: 0.9121\n",
      "Epoch 56, sentence 91/185, loss: 0.8602\n",
      "Epoch 56, sentence 181/185, loss: 0.5889\n",
      "Epoch 57, sentence 1/185, loss: 0.8994\n",
      "Epoch 57, sentence 91/185, loss: 0.8434\n",
      "Epoch 57, sentence 181/185, loss: 0.5791\n",
      "Epoch 58, sentence 1/185, loss: 0.8864\n",
      "Epoch 58, sentence 91/185, loss: 0.8233\n",
      "Epoch 58, sentence 181/185, loss: 0.5696\n",
      "Epoch 59, sentence 1/185, loss: 0.8736\n",
      "Epoch 59, sentence 91/185, loss: 0.8057\n",
      "Epoch 59, sentence 181/185, loss: 0.5604\n",
      "Epoch 60, sentence 1/185, loss: 0.8620\n",
      "Epoch 60, sentence 91/185, loss: 0.7885\n",
      "Epoch 60, sentence 181/185, loss: 0.5514\n",
      "Accuracy: 3679 / 4249, 0.8659\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\" Reads a vertical corpus with two columns: word, pos-tag.\n",
    "        Returns: list of tuples: [(words, tags)], one record per sentence.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    with open(\"data/train-small.txt\") as datafile:\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in datafile:\n",
    "            line = line.rstrip()\n",
    "            if not line:\n",
    "                data.append((words, tags))\n",
    "                words = []\n",
    "                tags = []\n",
    "            else:\n",
    "                word, tag = line.split()\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "    \n",
    "    \n",
    "            \n",
    "    return data\n",
    "  \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) < 2:\n",
    "        print('Usage: tagger.py TRAINSET', file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    main(sys.argv[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
